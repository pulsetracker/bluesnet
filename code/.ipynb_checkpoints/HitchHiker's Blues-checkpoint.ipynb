{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains the code to gerate data from the LSTM model and the trigram model.   If you \"run all\",\n",
    "it will load the appropriate libraries and generate some code.\n",
    "\n",
    "The last few lines in this notebook have all the good stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "# System setup.\n",
    "# Ensure that all of these libraries are available on the host machine\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "#from __future__ import print_function\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from __future__ import unicode_literals\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "import unicodedata\n",
    "import pronouncing\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Some of the libraries cause minor warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'corpus length:', 1227760)\n",
      "(u'Total number of Characters in Corpus:', 35)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and system-specific parameters\n",
    "# Related to the blues corpus\n",
    "\n",
    "pathRoot = '../data/'\n",
    "path = pathRoot + 'songsTextAll.txt'\n",
    "\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text))) # All berthe characters from the corpus\n",
    "print('Total number of Characters in Corpus:', len(chars))\n",
    "\n",
    "# Set up dictionaries mapping each character to its\n",
    "# index (and vice versa) in the chars set\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 100 # text is chopped up into chunks this big\n",
    "step = 3    # amount of overlap in chunks\n",
    "numIters = 5\n",
    "learning_rate = 0.2  # The learning rate for the model optimizer\n",
    "\n",
    "# Locations of trained LSTM models\n",
    "modelLoc = '../trainedLSTMModels'\n",
    "filename = 'trainedModel-7.hdf5'\n",
    "model = load_model(os.path.join(modelLoc,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# Library of useful functions for scraping and pre-processing training data\n",
    "###################################################################################\n",
    "\n",
    "\"\"\"\n",
    "# Implements the Hitchiker's Blues Final project for W266: Natural Language Processing\n",
    "#\n",
    "#\n",
    "#\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "import unicodedata\n",
    "import pronouncing\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "\tprint('\\tScraping PDF')\n",
    "\trsrcmgr = PDFResourceManager()\n",
    "\tretstr = StringIO()\n",
    "\t#codec = 'utf-8'\n",
    "\tcodec = 'ascii'\n",
    "\tlaparams = LAParams()\n",
    "\tdevice = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "\tfp = file(path, 'rb')\n",
    "\tinterpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\tpassword = \"\"\n",
    "\tmaxpages = 0\n",
    "\tcaching = True\n",
    "\tpagenos=set()\n",
    "\n",
    "\tfor page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "\t\tinterpreter.process_page(page)\n",
    "\n",
    "\ttext = retstr.getvalue()\n",
    "\n",
    "\tfp.close()\n",
    "\tdevice.close()\n",
    "\tretstr.close()\n",
    "\n",
    "\treturn text\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def purifyText(text, separatePunctuation=True, removePunctuation=False, removeWhitespace=False):\n",
    "\timport re\n",
    "\timport string\n",
    "\n",
    "\tprint '\\tPurifying text'\n",
    "\t# Remove ligatures and similar nonsense\n",
    "\tt = unicodedata.normalize(\"NFKD\",unicode(text)).encode('ascii',\"ignore\")\n",
    "\n",
    "\t# Remove page numbers and chapter identifiers\n",
    "\tpgnums = r\"\\n\\n\\d+\\n\\n\\x0c\"\n",
    "\tchptrs = r\"Chapter \\d+\"\n",
    "\tt = text\n",
    "\tt = re.sub(pgnums, '', t)\n",
    "\tt = re.sub(chptrs, '', t)\n",
    "\n",
    "\tif removePunctuation:\n",
    "\t\t# Remove punctuation\n",
    "\t\tpuncts = set(string.punctuation)\n",
    "\t\tt = ''.join(ch for ch in t if ch not in puncts)\n",
    "\n",
    "\tif separatePunctuation:\n",
    "\t\t'''Separate punctuation marks from their adjacent words so they are treated as separate tokens.'''\n",
    "\t\tpuncts = set(string.punctuation)\n",
    "\t\t#textList = t.split()\n",
    "\t\t#t = ' '.join(t)\n",
    "\t\tfor pMark in puncts:\n",
    "\t\t\tspacePMark = ' '+pMark+' '\n",
    "\t\t\tt.replace(pMark, spacePMark)\n",
    "\t\t\n",
    "\n",
    "\t# Remove whitespace\n",
    "\tif removeWhitespace:\n",
    "\t\tt = ''.join(t.split())\n",
    "\n",
    "\treturn t.lower()\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def buildDataset(filenames=['../data/guide1.pdf','../data/guide2.pdf','../data/guide3.pdf','../data/guide4.pdf','../data/guide5.pdf']):\n",
    "\n",
    "\ttrilogyText = ''\n",
    "\tfor ff in filenames:\n",
    "\t\tprint 'Adding file ', ff\n",
    "\t\ttry:\n",
    "\t\t\tthisText = convert_pdf_to_txt(ff)\n",
    "\t\t\tprettyText = purifyText(thisText)\n",
    "\t\t\ttrilogyText += prettyText\n",
    "\t\texcept Exception as E:\n",
    "\t\t\tprint 'Error processing file ', ff, '\\t\\t', E\n",
    "\n",
    "\toutFile = open('../data/dataset.txt','w')\n",
    "\n",
    "\tprint 'Writing to output file ', outFile\n",
    "\toutFile.write(trilogyText)\n",
    "\toutFile.close()\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def pullEqSubset(startChar=0, endChar = None , separation=100, fn = '../data/dataset.txt'):\n",
    "\n",
    "\timport numpy as np\n",
    "\n",
    "\td = open(fn).read()\n",
    "\n",
    "\tif not endChar:\n",
    "\t\tendChar=len(d)\n",
    "\t\n",
    "\n",
    "\tindices = np.arange(startChar,endChar,separation)\n",
    "\n",
    "\tprint 'Idx: ' ,indices\n",
    "\tsubset = [d[idx] for idx in indices]\n",
    "\n",
    "\treturn subset\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def parsewords(inputString, wordlist = '../code/wordlist.txt'):\n",
    "\tfrom math import log\n",
    "\t# Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "\twords = open(wordlist).read().split()\n",
    "\twordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "\tmaxword = max(len(x) for x in words)\n",
    "\n",
    "\tdef infer_spaces(s):\n",
    "\t\t\"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "\twithout spaces.\"\"\"\n",
    "\n",
    "\t\t# Find the best match for the i first characters, assuming cost has\n",
    "\t\t# been built for the i-1 first characters.\n",
    "\t\t# Returns a pair (match_cost, match_length).\n",
    "\t\tdef best_match(i):\n",
    "\t\t\tcandidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "\t\t\treturn min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "\t\t# Build the cost array.\n",
    "\t\tcost = [0]\n",
    "\t\tfor i in range(1,len(s)+1):\n",
    "\t\t\tc,k = best_match(i)\n",
    "\t\t\tcost.append(c)\n",
    "\n",
    "\t\t# Backtrack to recover the minimal-cost string.\n",
    "\t\tout = []\n",
    "\t\ti = len(s)\n",
    "\t\twhile i>0:\n",
    "\t\t\tc,k = best_match(i)\n",
    "\t\t\tassert c == cost[i]\n",
    "\t\t\tout.append(s[i-k:i])\n",
    "\t\t\ti -= k\n",
    "\n",
    "\t\treturn \" \".join(reversed(out))\n",
    "\n",
    "\t\n",
    "\treturn infer_spaces(inputString)\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def checkForEnglish(wordlist):\n",
    "\timport enchant\n",
    "\tdic = enchant.Dict(\"en_US\")\n",
    "\t\n",
    "\tif type(wordlist) == type(''):\n",
    "\t\twordlist = list(wordlist)\n",
    "\n",
    "\n",
    "\tisEnglish = [dic.check(word) for word in wordlist if len(word)>1]\n",
    "\t\n",
    "\treturn isEnglish\n",
    "\n",
    "#####################################################################################################################\n",
    "def getNumSyllables(word): \n",
    "\t'''Returns the number of syllables in the string word.\n",
    "\tNote:  word must be in the cmudict.  If it isn't, function will return 0 syllables.\n",
    "\n",
    "\tAn adaptation of stackoverflow.com/questions/5087493/to-find-the-number-of-syllables-in-a-word\n",
    "\tto make it both readable and make it work\n",
    "\t'''\n",
    "\timport curses \n",
    "\tfrom curses.ascii import isdigit \n",
    "\timport nltk\n",
    "\tfrom nltk.corpus import cmudict \n",
    "\td = cmudict.dict() # a dictionary of pronounciation\n",
    "\n",
    "\tsyllable_count = 0\n",
    "\tif word.lower() in d.keys():\n",
    "\t\tfor sounds in d[word.lower()][0]:  #d['astronomy'] returns [u'AH0', u'S', u'T', u'R', u'AA1', u'N', u'AH0', u'M', u'IY0']\n",
    "\t\t\tfor s in sounds:\n",
    "\t\t\t\ts_ascii = str(s)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif isdigit(s_ascii[-1]):\t\t#syllables have a digit on them\n",
    "\t\t\t\t\tsyllable_count += 1\n",
    "\n",
    "\t#print d[word.lower()]\n",
    "\treturn syllable_count\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def getRhymes(word):\n",
    "\t\n",
    "\trhymes = pronouncing.rhymes(word)\n",
    "\n",
    "\treturn rhymes\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "def buildNGrams(fullText, n=3, shortSample = False, printSample = False):\n",
    "\tfrom nltk import ngrams\n",
    "\n",
    "\tif shortSample:\n",
    "\t\tfullText = \" \".join(fullText.split()[0:10000])\n",
    "\n",
    "\tnGrams = ngrams(fullText.split(), n) # Returns a list of tuples\n",
    "\t\n",
    "\tif printSample:\n",
    "\t\tfor g in nGrams[0:10]:\n",
    "\t\t  print g\n",
    "\n",
    "\n",
    "\treturn nGrams\n",
    "#####################################################################################################################\n",
    "import pickle\n",
    "def getNGramAdams(buildNewDataSet=False):\n",
    "\timport os\n",
    "\t\n",
    "\tif buildNewDataSet or ('dataset.txt' not in os.listdir('../data')):\n",
    "\t\tprint 'dataset.txt not found.  Rebuilding'\n",
    "\t\tbuildDataset()\n",
    "\n",
    "\ttext = open('../data/dataset.txt').read()\n",
    "\n",
    "\tngGen = buildNGrams(text, shortSample=True, printSample=False)\n",
    "\n",
    "\tprint 'Writing n-grams to pickle'\n",
    "\tng = [n for n in ngGen]\n",
    "\n",
    "\tpickle.dump(ng, open( '../data/ngrams.txt','w'))\n",
    "\t\n",
    "#####################################################################################################################\n",
    "\n",
    "class bluesTune():\n",
    "\n",
    "\t\"\"\" Example song:\n",
    "\n",
    "\t  Aker   1       Akers, Garfield\n",
    "\t\\C Text transcribed from discography listed in and edited for publication in\n",
    "\t\\C Michael Taft, \\iBlues Lyric Poetry: An Anthology\\r. New York: Garland\n",
    "\t\\C Publishing, Inc., 1983. See also, Michael Taft, \\iBlues Lyric Poetry: A\n",
    "\t\\C Concordance\\r. New York: Garland Publishing, Inc., 1984.\n",
    "\t\\C    title: Cottonfield Blues-+-Part 1\n",
    "\t\\C    place and date: Memphis, c. 23 Sept. 1929\n",
    "\t\\C    record numbers: (M-201- ) Vo-1442 OJL-2\n",
    "\n",
    "\tI said look a-here mama :\n",
    "\t what in the world are you trying to do\n",
    "\tYou want to make me love you : \n",
    "\t\tyou going to break my heart in two\n",
    "\n",
    "\tI said you don't want me : \n",
    "\t\twhat made you want to lie\n",
    "\tNow the day you quit me fair brown :\n",
    "\t\t baby that's the day you die\n",
    "\n",
    "\tI'd rather see you dead :\n",
    "\t\t buried in some cypress grove\n",
    "\tThan to hear some gossip mama :\n",
    "\t\t that she had done you so\n",
    "\n",
    "\tIt was early one morning :\n",
    "\t\t just about the break of day\n",
    "\tAnd along brownskin coming :\n",
    "\t\t man and drove me away\n",
    "\n",
    "\tLord my baby quit me :\n",
    "\t\t she done set my trunk outdoors\n",
    "\tThat put the poor boy wandering :\n",
    "\t\t Lord along the road\n",
    "\n",
    "\tI said trouble here mama :\n",
    "\t\t and trouble everywhere you go\n",
    "\tAnd it's trouble here mama :\n",
    "\t\t baby good gal I don't know\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, rawText=None):\n",
    "\n",
    "\t\tself.rawText = \"\\L\"+rawText.strip()\n",
    "\t\tself.artist = ''\n",
    "\t\tself.title = ''\n",
    "\t\tself.time = ''\n",
    "\t\tself.place = ''\n",
    "\t\tself.record_numbers = ''\n",
    "\t\tself.fullHeaderText = ''\n",
    "\t\tself.fullLyricText = ''\n",
    "\t\tself.shorthand_code = ''\n",
    "\n",
    "\t\tself.format = ''  # Something like AABA\n",
    "\t\tself.stanzas = []  # one element in the array for each two lines (1 stanza) of the song\n",
    "\n",
    "\t\tif self.rawText:\n",
    "\t\t\tself.parseSong()\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\toutText = '\\n'\n",
    "\t\tfor stz in self.stanzas:\n",
    "\t\t\toutText+=stz.replace('::','\\n').replace(':','\\n\\t')+'\\n\\n'\n",
    "\t\t#return '\\n\\n'+self.fullLyricText.replace('::','\\n2').replace(':','\\n\\t')\n",
    "\t\treturn outText\n",
    "\n",
    "\tdef sents(self):\n",
    "\t\tsent = []\n",
    "\t\tfor stz in self.stanzas:\n",
    "\t\t\tfor word in stz.split():\n",
    "\t\t\t\tsent.append(word)\n",
    "\t\t\tsent.append(':::')\n",
    "\t\t#return [stz.split() for stz in self.stanzas]\n",
    "\t\treturn sent\n",
    "\n",
    "\n",
    "\tdef parseSong(self):\n",
    "\t\tif self.rawText: \n",
    "\n",
    "\t\t\tsplitRaw = self.rawText.split('\\\\')\n",
    "\n",
    "\t\t\tself.shorthand_code = \" \".join(splitRaw[0].strip().split()[0:2])\n",
    "\t\t\tself.artist = \" \".join(splitRaw[0].strip().split()[2:])\n",
    "\n",
    "\t\t\tfor ss in self.rawText.split('\\n\\n'):\n",
    "\t\t\t\tif ss.startswith('\\\\'):\n",
    "\t\t\t\t\tself.fullHeaderText+= ss+\"\\n\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fullLyricText += ss+\"\\n\"\n",
    "\n",
    "\t\n",
    "\t\t\t# join each pair of lines into a stanza with lines separated by ::\n",
    "\t\t\t'''\tI said trouble here mama : and trouble everywhere you go\n",
    "\t\t\t\tAnd it's trouble here mama : baby good gal I don't know\n",
    "\n",
    "\t\t\t\t\tbecomes\n",
    "\n",
    "\t\t\t\tI said trouble here mama : and trouble everywhere you go :: And it's trouble here mama : baby good gal I don't know\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\t'''\n",
    "\n",
    "\t\t\tstack = []\n",
    "\t\t\tintText = self.fullLyricText.split('\\n')\n",
    "\n",
    "\t\t\tfor ll in list(enumerate(intText)):\n",
    "\t\t\t\tprint 'll: ', ll\n",
    "\t\t\t\tif ll[0]%2==0:\n",
    "\t\t\t\t\tstack.append(ll[1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.stanzas.append(stack.pop()+' :: '+ll[1])\n",
    "\n",
    "\n",
    "def loadBluesSongs(process=True):\n",
    "\t\"\"\"\n",
    "\tprocess:  Whethter to process out caps and weird characters\n",
    "\t\"\"\"\n",
    "\n",
    "\tfiles = ['../data/bluesLyrics/1409/blues1.1409', \n",
    "\t\t\t\t'../data/bluesLyrics/1409/blues2.1409']\n",
    "\n",
    "\tsongs = []\n",
    "\n",
    "\tfor ff in files:\n",
    "\t\ttry:\n",
    "\t\t\tthisFile = open(ff).readlines()\n",
    "\t\t\tthisBlock = \"\\n\".join(thisFile)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tthisSongs = thisBlock.split('\\L')\n",
    "\n",
    "\t\t\tfor song in thisSongs:\n",
    "\t\t\t\tif process:\n",
    "\t\t\t\t \tsong = song.replace('*','')\n",
    "\t\t\t\t \tsong = song.lower()\n",
    "\t\n",
    "\t\t\t\tsongs.append(bluesTune(rawText = song))\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint 'Error importing file ', ff, ' || ', e\n",
    "\n",
    "\n",
    "\n",
    "\treturn songs\n",
    "\n",
    "def buildEmbeddings():\n",
    "\tfrom gensim.models import Word2Vec\n",
    "\t#from nltk.corpus import brown, movie_reviews, treebank\n",
    "\t#>>> b = Word2Vec(brown.sents())\n",
    "\t#>>> mr = Word2Vec(movie_reviews.sents())\n",
    "\t#>>> t = Word2Vec(treebank.sents())\n",
    " \n",
    "\t#>>> b.most_similar('money', topn=5)\n",
    "\t#[('pay', 0.6832243204116821), ('ready', 0.6152011156082153), ('try', 0.5845392942428589), ('care', 0.5826011896133423), ('move', 0.5752171277999878)]\n",
    "\n",
    "\tsongs = loadBluesSongs()\n",
    "\n",
    "\tallSents = [ss.sents() for ss in songs]\n",
    "\tallText = \"\\n\".join([ss.fullLyricText for ss in songs])\n",
    "\tem = Word2Vec(allSents)\n",
    "\n",
    "\t\n",
    "def genLineBigram(in_songs = None, startWord=None, num=15):\n",
    "\timport random\n",
    "\timport nltk\n",
    "\n",
    "\tif not in_songs:\n",
    "\t\tsongs = loadBluesSongs()\n",
    "\telse:\n",
    "\t\tsongs = in_songs\n",
    "\n",
    "\tsongWords = \" \".join([ss.fullLyricText for ss in songs]).split()\n",
    "\tbigrams = nltk.bigrams(songWords)\n",
    "\tcfdist = nltk.ConditionalFreqDist(bigrams)\n",
    "\tcpdist = nltk.ConditionalProbDist(cfdist, nltk.ELEProbDist)\n",
    "\n",
    "\tif startWord == None:\n",
    "\t\tword = random.choice(cpdist.keys())\n",
    "\telse:\n",
    "\t\tword = startWord\n",
    "\n",
    "\toutLine = ''\n",
    "\tfor i in range(num):\n",
    "\t\toutLine += word + ' '\n",
    "\t\tword = cpdist[word].generate()\n",
    "\n",
    "\t#print outLine\n",
    "\n",
    "\treturn outLine,cpdist\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "def genLine(in_songs = None, startContext=None,n=2, num=15):\n",
    "\timport random\n",
    "\timport nltk\n",
    "\n",
    "\tif not in_songs:\n",
    "\t\tsongs = loadBluesSongs()\n",
    "\telse:\n",
    "\t\tsongs = in_songs\n",
    "\n",
    "\tsongWords = \" \".join([ss.fullLyricText for ss in songs]).split()\n",
    "\t# bigrams = nltk.bigrams(songWords)\n",
    "\tngrams = nltk.ngrams(songWords, n)\n",
    "\t#condition_pairs = (((w0, w1), w2) for w0, w1, w2 in ngrams)\n",
    "\tcondition_pairs = [((w[0:-1]),w[-1]) for w in ngrams]\n",
    "\tcfdist = nltk.ConditionalFreqDist(condition_pairs)\n",
    "\tcpdist = nltk.ConditionalProbDist(cfdist, nltk.ELEProbDist)\n",
    "\n",
    "\tif startContext == None:\n",
    "\t\tcontext = tuple([random.choice(cpdist.keys()) for nn in range(n-1)])\n",
    "\telse:\n",
    "\t\tcontext = startContext\n",
    "\n",
    "\t# All the start words should be a tuple, but users are unlikely to enter it that way\n",
    "\tif type(context)==type(''):\n",
    "\t\tcontext = tuple(context.split())\n",
    "\n",
    "\n",
    "\toutLineList = []\n",
    "\tfor i in range(num):\n",
    "\t\tprint i, num, outLineList #context\n",
    "\t\toutLineList += list(context)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tnewWord = cpdist[context].generate()\n",
    "\t\texcept:\n",
    "\t\t\tnewWord = random.choice(cpdist.keys())\n",
    "\n",
    "\t\tnewContext = (outLineList[-(n-1)],newWord)\n",
    "\t\tcontext=newContext\n",
    "\n",
    "\t\t#print outLineList\n",
    "\t#return \" \".join(outLineList)\n",
    "\treturn outLineList\n",
    "\n",
    "class bluesStanza(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, words=[]):\n",
    "        from nltk.tokenize import WhitespaceTokenizer as WST\n",
    "        self.pickledSongsLoc = '../data/unigramProbs.pickle'\n",
    "        self.line1 = []\n",
    "        self.line2 = []\n",
    "        self.line3 = []\n",
    "        self.line4 = []\n",
    "        \n",
    "        try:\n",
    "            bluesSongs = pickle.load(open(self.pickledSongsLoc))\n",
    "        except:\n",
    "            bluesSongs = loadBluesSongs()\n",
    "            pickle.dump(bluesSongs, open(self.pickledSongsLoc,'w'))\n",
    "                    \n",
    "        bluesWords = \" \".join([xx.__str__() for xx in bluesSongs])\n",
    "        wst = WST()\n",
    "        \n",
    "        self.words = wst.tokenize(bluesWords)\n",
    "        self.fdist = nltk.FreqDist(self.words)\n",
    "        self.bluesUnigramProbs = nltk.MLEProbDist(self.fdist)\n",
    "\n",
    "        # Build trigram model\n",
    "        tris = []\n",
    "        for x in list(nltk.trigrams(self.words)):\n",
    "            tris.append( ((x[0],x[1]),x[2]) )\n",
    "\n",
    "        self.tf = nltk.ConditionalFreqDist(tris)\n",
    "        #self.pd = nltk.MLEProbDist(self.tf)\n",
    "        \n",
    "        pd = dict()\n",
    "        for cond in self.tf.conditions():\n",
    "            pd[cond] = nltk.MLEProbDist(self.tf[cond])\n",
    "        self.pd = pd\n",
    "        \n",
    "        #words = list(set(list(words)))\n",
    "        \n",
    "        if len(words) == 2:\n",
    "            self.startContext = tuple(words)\n",
    "        else :\n",
    "            self.startContext = tuple(self.getBluesWords(2))\n",
    "        \n",
    "    def getBluesWords(self,n=1):\n",
    "        \n",
    "        wordList = []\n",
    "        if n < 1:\n",
    "            n=1\n",
    "            \n",
    "        if n>1:\n",
    "            return [self.bluesUnigramProbs.generate() for ii in range(n)]\n",
    "        \n",
    "        else:\n",
    "            return self.bluesUnigramProbs.generate()\n",
    "        \n",
    "    def __str__(self):\n",
    "        outLine = \" \".join(self.line1)\n",
    "        outLine += '\\n\\t\\t' + \" \".join(self.line2)\n",
    "        outLine += '\\n' + \" \".join(self.line3)\n",
    "        outLine += '\\n\\t\\t' + \" \".join(self.line4)\n",
    "        \n",
    "        return outLine\n",
    "    \n",
    "    def genStanza(self):\n",
    "        self.line1 = self.genLine(self.startContext, start=True)\n",
    "        self.line2 = self.genLine(tuple(self.line1[-2:]))\n",
    "        self.line3 = self.genLine(tuple(self.line2[-2:]))\n",
    "        self.line4 = self.genLine(tuple(self.line3[-2:]))\n",
    "        self.forceRhyme()\n",
    "    \n",
    "    def genLine(self, startContextTuple, minSyl = 5, start=False):\n",
    "        # Probably should generate number of syllables based on the data\n",
    "        import numpy as np\n",
    "        \n",
    "        context = startContextTuple\n",
    "\n",
    "        if start==True:\n",
    "            thisSylCount = np.sum([getNumSyllables(x) for x in context])\n",
    "            outLineList = list(context)\n",
    "        else:\n",
    "            outLineList = []\n",
    "            thisSylCount = 0\n",
    "\n",
    "        while thisSylCount < minSyl:\n",
    "            #print i,' of ',num, outLineList #context\n",
    "            try:\n",
    "                newWord = self.pd[context].generate()\n",
    "            except:\n",
    "            # maybe this should revert to bigram?\n",
    "                newWord = self.getBluesWords(1)\n",
    "\n",
    "            newContext = (context[-1],newWord)\n",
    "            outLineList.append(newWord)\n",
    "            try: \n",
    "                thisSylCount += getNumSyllables(newWord)\n",
    "            except:\n",
    "                thisSylCount += 2\n",
    "            #print newContext\n",
    "            context=newContext\n",
    "\n",
    "        #print \" \".join(outLineList)\n",
    "        return outLineList\n",
    "    \n",
    "    def getCorpusRhymePairs():\n",
    "        pass\n",
    "        \n",
    "    def forceRhyme(self, line2=None, line4=None):\n",
    "        import pronouncing\n",
    "        import random\n",
    "        \n",
    "        # If lines are not provided, assume user\n",
    "        # wants to use the lines in the class attributes\n",
    "        if line2==None:\n",
    "            line2 = self.line2\n",
    "            \n",
    "        if line4==None:\n",
    "            line4 = self.line4\n",
    "\n",
    "\n",
    "        # Type 1:  If model made a rhyming word, keep it\n",
    "        rhymeList = pronouncing.rhymes(line2[-1])\n",
    "        if line4[-1] in rhymeList:\n",
    "            print 'Type 1'\n",
    "            self.line4 = line4\n",
    "            return None\n",
    "        \n",
    "        # Type 2: Use a word from a corpus rhyme pair\n",
    "        \n",
    "        # Type 3: Use a rhyming word from the trigram model\n",
    "        thisContext = tuple(line4[-3:-1])\n",
    "        contextWords = set(self.pd[thisContext].samples())\n",
    "        rhymeWords = set(rhymeList)\n",
    "        contextRhymes = contextWords.intersection(rhymeWords)\n",
    "        if len(contextRhymes)>0:\n",
    "            print 'Type 3'\n",
    "            rhymeword = random.choice(list(contextRhymes))\n",
    "            self.line4 =  line4[0:-1]+[rhymeword]\n",
    "            return None\n",
    "        \n",
    "        # Type 4: Use a rhyming word from the corpus\n",
    "        corpusRhymes = set(cc.words).intersection(rhymeWords)\n",
    "        if len(corpusRhymes) > 0:\n",
    "            print 'Type 4'\n",
    "            rhymeword = random.choice(list(corpusRhymes))\n",
    "            self.line4 = line4[0:-1]+[rhymeword]\n",
    "            return None\n",
    "        \n",
    "        # Type 5: Return the original word (everything rhymes with itself)\n",
    "        print 'Type 5'\n",
    "        self.line4 = line4[0:-1] + [line2[-1]]\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 4\n",
      "i harry mama\n",
      "\t\tdo you no more i'm\n",
      "in my door brown you\n",
      "\t\twon't feel so sad eee seldom\n",
      "Type 3\n",
      "me said the same damn\n",
      "\t\ttrack lord today has\n",
      "been refused can't have\n",
      "\t\tyour fun you just as\n",
      "Type 4\n",
      "you up in the air\n",
      "\t\ti'm going to grab\n",
      "me a trip down to\n",
      "\t\tmemphis won't find crab\n",
      "Type 4\n",
      "say to you now baby\n",
      "\t\tand i walked down to\n",
      "the river get me\n",
      "\t\ta bulldog bark stew\n",
      "Type 4\n",
      "somebody to help\n",
      "\t\tyou because he didn't\n",
      "have no hat asking\n",
      "\t\tall the time i forbidden\n",
      "Type 4\n",
      "if texas children\n",
      "\t\tgive got the slow-driving blues blue\n",
      "as blue as i got\n",
      "\t\tone teeth solid into\n",
      "Type 3\n",
      "riding got her another\n",
      "\t\tman lord she'll take bad\n",
      "treatment i just tell\n",
      "\t\tme this morning had\n",
      "Type 4\n",
      "seldom you my money\n",
      "\t\tand baby sitting\n",
      "on a saturday\n",
      "\t\tnight spender and fitting\n",
      "Type 4\n",
      "of got lord right sweet\n",
      "\t\tmama got to do\n",
      "now just see my black\n",
      "\t\tgal walking down due\n",
      "Type 4\n",
      "mmm is depot so right\n",
      "\t\tyou know the blues all\n",
      "around my head and\n",
      "\t\tcry i did more crawl\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "# Generate a trigram model stanzas:\n",
    "# (On most machines, this is is kind of slow)\n",
    "###################################################################################\n",
    "numStanzasToGenerate = 4\n",
    "for i in range(numStanzasToGenerate):\n",
    "    cc=bluesStanza()\n",
    "    cc.genStanza()\n",
    "    print cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bluesnet LSTM Model Activated. \n",
      "(u'----- diversity:', 0.5)\n",
      "----- Generating with seed: \"                                                                 so long and thanks for all the fish\"\n",
      "\n",
      "*****************************************************************\n",
      "\n",
      "                                                                 so long and thanks for all the fish \n",
      "\t and stay and trouble \n",
      "\t i done to my daddy \n",
      "\t the gal was the way it \n",
      "\t long i'm down \n",
      "\t say my women don't be me \n",
      "\t and i was my \n",
      "\t and i got to be look around my right \n",
      " when i done \n",
      " i was my cown \n",
      " i ain't can here and only \n",
      "\t where don't have come me \n",
      "\t and the lone where i say \n",
      "\t she wouldn't say iy bottrens you \n",
      "\t i got to the walls \n",
      "\t before she was to some of your sun \n",
      "\t you ain't sat"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "# Generate characters from the LSTM model\n",
    "###################################################################################\n",
    "\n",
    "n = 400         # number of characters to generate\n",
    "diversity = 0.5 # a measure of the \"randomality\" of the randomly generated letters\n",
    "                # should be in range (0,1) with larger values making more \"diverse\"\n",
    "                # outputs\n",
    "seed='so long and thanks for all the fish' # the initial input to the LSTM model\n",
    "\n",
    "# Ensure seed has 100 characters\n",
    "if len(seed) < 100:\n",
    "    seed = ' '*(100-len(seed))+seed\n",
    "elif len(seed) > 100:\n",
    "    seed = seed[-100:]\n",
    "\n",
    "# A helper function for word generation\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def genBluesCharacters(n=400,diversity=0.5, seed='so long and thanks for all the fish', verbose = True):\n",
    "        \"\"\"\n",
    "        diversity in (0,1)--how much variation when generating text\n",
    "        n -- how many characters to generate\n",
    "        seed -- starting text    \n",
    "        \"\"\"\n",
    "        maxlen=100\n",
    "        chars = ['\\t', '\\n', ' ', \"'\", ',', '-', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "        indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\n\\nBluesnet LSTM Model Activated. ')\n",
    "            print('----- diversity:', diversity)\n",
    "            print('----- Generating with seed: \"' + seed + '\"')\n",
    "            print('\\n*****************************************************************\\n')\n",
    "        \n",
    "        output = seed\n",
    "        generated = ''\n",
    "        generated += seed\n",
    "        \n",
    "        if verbose:\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(n):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(output):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            output = output[1:] + next_char\n",
    "\n",
    "            if verbose:\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        return generated\n",
    "    \n",
    "outTxt = genBluesCharacters(n=n,diversity=diversity, seed=seed,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'and the one to the cried', u\"i'm going to be stay\", u'what my buddy', u'the rooked the blues on a women']\n",
      "Type 4\n",
      "and the one to the cried\n",
      "\t\ti'm going to be stay\n",
      "what my buddy\n",
      "\t\tthe rooked the blues on a say\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "#How to use rhyme forcing on LSTM output\n",
    "###################################################################################\n",
    "\n",
    "s = '''and the one to the cried \n",
    "\t i'm going to be stay \n",
    " what my buddy \n",
    "\t the rooked the blues on a women'''\n",
    "s=[line.strip() for line in s.split('\\n')]\n",
    "print s\n",
    "dd=bluesStanza() \n",
    "dd.line1 = s[0].split()\n",
    "dd.line2 = s[1].split()\n",
    "dd.line3 = s[2].split()\n",
    "dd.line4 = s[3].split()\n",
    "dd.forceRhyme()\n",
    "print dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
